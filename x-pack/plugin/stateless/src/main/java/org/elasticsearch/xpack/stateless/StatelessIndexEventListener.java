/*
 * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one
 * or more contributor license agreements. Licensed under the Elastic License
 * 2.0; you may not use this file except in compliance with the Elastic License
 * 2.0.
 */

package org.elasticsearch.xpack.stateless;

import org.apache.lucene.index.SegmentInfos;
import org.apache.lucene.store.AlreadyClosedException;
import org.apache.lucene.store.IOContext;
import org.elasticsearch.action.ActionListener;
import org.elasticsearch.action.support.SubscribableListener;
import org.elasticsearch.action.support.ThreadedActionListener;
import org.elasticsearch.cluster.metadata.IndexReshardingMetadata;
import org.elasticsearch.cluster.project.ProjectResolver;
import org.elasticsearch.cluster.routing.RecoverySource;
import org.elasticsearch.cluster.routing.ShardRoutingState;
import org.elasticsearch.common.blobstore.BlobContainer;
import org.elasticsearch.common.blobstore.BlobPath;
import org.elasticsearch.common.blobstore.BlobStore;
import org.elasticsearch.common.settings.ClusterSettings;
import org.elasticsearch.common.util.concurrent.ConcurrentCollections;
import org.elasticsearch.core.Tuple;
import org.elasticsearch.index.IndexSettings;
import org.elasticsearch.index.engine.Engine;
import org.elasticsearch.index.engine.NoOpEngine;
import org.elasticsearch.index.seqno.SequenceNumbers;
import org.elasticsearch.index.shard.IndexEventListener;
import org.elasticsearch.index.shard.IndexShard;
import org.elasticsearch.index.shard.IndexShardState;
import org.elasticsearch.index.shard.ShardId;
import org.elasticsearch.index.store.Store;
import org.elasticsearch.index.translog.Translog;
import org.elasticsearch.indices.recovery.RecoveryState;
import org.elasticsearch.logging.LogManager;
import org.elasticsearch.logging.Logger;
import org.elasticsearch.threadpool.ThreadPool;
import org.elasticsearch.xpack.stateless.cache.SharedBlobCacheWarmingService;
import org.elasticsearch.xpack.stateless.cache.StatelessSharedBlobCacheService;
import org.elasticsearch.xpack.stateless.commits.BatchedCompoundCommit;
import org.elasticsearch.xpack.stateless.commits.BlobFile;
import org.elasticsearch.xpack.stateless.commits.BlobFileRanges;
import org.elasticsearch.xpack.stateless.commits.HollowShardsService;
import org.elasticsearch.xpack.stateless.commits.StatelessCommitService;
import org.elasticsearch.xpack.stateless.commits.StatelessCompoundCommit;
import org.elasticsearch.xpack.stateless.engine.HollowIndexEngine;
import org.elasticsearch.xpack.stateless.engine.IndexEngine;
import org.elasticsearch.xpack.stateless.engine.PrimaryTermAndGeneration;
import org.elasticsearch.xpack.stateless.engine.SearchEngine;
import org.elasticsearch.xpack.stateless.engine.translog.TranslogReplicator;
import org.elasticsearch.xpack.stateless.lucene.BlobStoreCacheDirectory;
import org.elasticsearch.xpack.stateless.lucene.IndexBlobStoreCacheDirectory;
import org.elasticsearch.xpack.stateless.lucene.IndexDirectory;
import org.elasticsearch.xpack.stateless.lucene.SearchDirectory;
import org.elasticsearch.xpack.stateless.objectstore.ObjectStoreService;
import org.elasticsearch.xpack.stateless.recovery.RecoveryCommitRegistrationHandler;
import org.elasticsearch.xpack.stateless.recovery.RegisterCommitResponse;
import org.elasticsearch.xpack.stateless.reshard.SplitSourceService;
import org.elasticsearch.xpack.stateless.reshard.SplitTargetService;

import java.io.IOException;
import java.io.UncheckedIOException;
import java.util.Map;
import java.util.Set;
import java.util.concurrent.Executor;
import java.util.stream.Collectors;

import static org.elasticsearch.index.shard.StoreRecovery.bootstrap;
import static org.elasticsearch.xpack.stateless.cache.SharedBlobCacheWarmingService.Type.INDEXING;
import static org.elasticsearch.xpack.stateless.cache.SharedBlobCacheWarmingService.Type.SEARCH;
import static org.elasticsearch.xpack.stateless.commits.BlobFileRanges.computeBlobFileRanges;
import static org.elasticsearch.xpack.stateless.commits.StatelessCompoundCommit.HOLLOW_TRANSLOG_RECOVERY_START_FILE;
import static org.elasticsearch.xpack.stateless.commits.StatelessCompoundCommit.TRANSLOG_RECOVERY_START_FILE;

class StatelessIndexEventListener implements IndexEventListener {

    private static final Logger logger = LogManager.getLogger(StatelessIndexEventListener.class);

    private final ThreadPool threadPool;
    private final StatelessCommitService statelessCommitService;
    private final ObjectStoreService objectStoreService;
    private final TranslogReplicator translogReplicator;
    private final RecoveryCommitRegistrationHandler recoveryCommitRegistrationHandler;
    private final SharedBlobCacheWarmingService warmingService;
    private final StatelessSharedBlobCacheService cacheService;
    private final HollowShardsService hollowShardsService;
    private final SplitTargetService splitTargetService;
    private final SplitSourceService splitSourceService;
    private final ProjectResolver projectResolver;
    private final Executor bccHeaderReadExecutor;
    private final boolean useInternalFilesReplicatedContentForSearchShards;

    StatelessIndexEventListener(
        ThreadPool threadPool,
        StatelessCommitService statelessCommitService,
        ObjectStoreService objectStoreService,
        TranslogReplicator translogReplicator,
        RecoveryCommitRegistrationHandler recoveryCommitRegistrationHandler,
        SharedBlobCacheWarmingService warmingService,
        HollowShardsService hollowShardsService,
        SplitTargetService splitTargetService,
        SplitSourceService splitSourceService,
        ProjectResolver projectResolver,
        Executor bccHeaderReadExecutor,
        ClusterSettings clusterSettings,
        StatelessSharedBlobCacheService cacheService
    ) {
        this.threadPool = threadPool;
        this.statelessCommitService = statelessCommitService;
        this.objectStoreService = objectStoreService;
        this.translogReplicator = translogReplicator;
        this.recoveryCommitRegistrationHandler = recoveryCommitRegistrationHandler;
        this.warmingService = warmingService;
        this.hollowShardsService = hollowShardsService;
        this.splitTargetService = splitTargetService;
        this.splitSourceService = splitSourceService;
        this.projectResolver = projectResolver;
        this.bccHeaderReadExecutor = bccHeaderReadExecutor;
        this.cacheService = cacheService;
        this.useInternalFilesReplicatedContentForSearchShards = clusterSettings.get(
            SearchEngine.STATELESS_SEARCH_USE_INTERNAL_FILES_REPLICATED_CONTENT
        );
    }

    @Override
    public void afterFilesRestoredFromRepository(IndexShard indexShard) {
        final var store = indexShard.store();
        store.incRef();
        try {
            final var userData = store.readLastCommittedSegmentsInfo().getUserData();
            final String startFile = userData.get(TRANSLOG_RECOVERY_START_FILE);
            if (startFile == null) {
                return;
            }
            final var startFileValue = Long.parseLong(startFile);
            if (startFileValue == HOLLOW_TRANSLOG_RECOVERY_START_FILE) {
                return;
            }
            final var currentNodeStartFileForNextCommit = translogReplicator.getMaxUploadedFile() + 1;
            if (startFileValue != currentNodeStartFileForNextCommit) {
                store.associateIndexWithNewUserKeyValueData(TRANSLOG_RECOVERY_START_FILE, Long.toString(currentNodeStartFileForNextCommit));
            }
        } catch (IOException e) {
            throw new UncheckedIOException(e);
        } finally {
            store.decRef();
        }
    }

    @Override
    public void beforeIndexShardRecovery(IndexShard indexShard, IndexSettings indexSettings, ActionListener<Void> listener) {
        final Store store = indexShard.store();
        try {
            store.incRef();
            boolean success = false;
            try {
                final var projectId = projectResolver.getProjectId();
                final var shardId = indexShard.shardId();
                assert objectStoreService.assertProjectIdAndShardIdConsistency(projectId, shardId);

                final BlobStore blobStore = objectStoreService.getProjectBlobStore(projectId);
                final BlobPath shardBasePath = objectStoreService.shardBasePath(projectId, shardId);
                final BlobContainer existingBlobContainer = hasNoExistingBlobContainer(indexShard.recoveryState().getRecoverySource())
                    ? null
                    : blobStore.blobContainer(shardBasePath);

                BlobStoreCacheDirectory.unwrapDirectory(store.directory())
                    .setBlobContainer(primaryTerm -> blobStore.blobContainer(shardBasePath.add(String.valueOf(primaryTerm))));

                var releaseAfterListener = ActionListener.releaseAfter(listener, store::decRef);
                if (indexShard.routingEntry().isSearchable()) {
                    beforeRecoveryOnSearchShard(indexShard, existingBlobContainer, releaseAfterListener);
                } else {
                    if (IndexReshardingMetadata.isSplitTarget(shardId, indexSettings.getIndexMetadata().getReshardingMetadata())) {
                        splitTargetService.startSplitTargetShardRecovery(
                            indexShard,
                            indexSettings.getIndexMetadata(),
                            new ThreadedActionListener<>(
                                threadPool.generic(),
                                releaseAfterListener.delegateFailureAndWrap(
                                    (listener1, unused) -> beforeRecoveryOnIndexingShard(
                                        indexShard,
                                        existingBlobContainer,
                                        releaseAfterListener
                                    )
                                )
                            )
                        );
                    } else {
                        beforeRecoveryOnIndexingShard(indexShard, existingBlobContainer, releaseAfterListener);
                    }
                }
                success = true;
            } finally {
                if (success == false) {
                    store.decRef();
                }
            }
        } catch (Exception e) {
            listener.onFailure(e);
        }
    }

    private static boolean hasNoExistingBlobContainer(RecoverySource recoverySource) {
        return recoverySource == RecoverySource.EmptyStoreRecoverySource.INSTANCE
            || recoverySource instanceof RecoverySource.SnapshotRecoverySource;
    }

    private static void logBootstrapping(IndexShard indexShard, BatchedCompoundCommit latestCommit) {
        logger.info(
            "{} with UUID [{}] bootstrapping [{}] shard on primary term [{}] with {} from object store ({})",
            indexShard.shardId(),
            indexShard.shardId().getIndex().getUUID(),
            indexShard.routingEntry().role(),
            indexShard.getOperationPrimaryTerm(),
            latestCommit != null ? latestCommit.lastCompoundCommit().toShortDescription() : "empty commit",
            describe(indexShard.recoveryState())
        );
    }

    private static void logBootstrapping(
        IndexShard indexShard,
        StatelessCompoundCommit latestCommit,
        PrimaryTermAndGeneration latestUploaded
    ) {
        assert indexShard.routingEntry().isPromotableToPrimary() == false;
        assert latestCommit != null;
        logger.info(
            "{} with UUID [{}] bootstrapping [{}] shard on primary term [{}] with {} and latest uploaded {} from indexing shard ({})",
            indexShard.shardId(),
            indexShard.shardId().getIndex().getUUID(),
            indexShard.routingEntry().role(),
            indexShard.getOperationPrimaryTerm(),
            latestCommit.toShortDescription(),
            latestUploaded,
            describe(indexShard.recoveryState())
        );
    }

    private static String describe(RecoveryState recoveryState) {
        return recoveryState.getRecoverySource() == RecoverySource.PeerRecoverySource.INSTANCE
            ? recoveryState.getRecoverySource() + " from " + recoveryState.getSourceNode().getName()
            : recoveryState.getRecoverySource().toString();
    }

    private void beforeRecoveryOnIndexingShard(IndexShard indexShard, BlobContainer shardContainer, ActionListener<Void> listener) {
        assert indexShard.store().refCount() > 0 : indexShard.shardId();
        assert indexShard.routingEntry().isPromotableToPrimary();
        SubscribableListener.<ObjectStoreService.IndexingShardState>newForked(l -> {
            if (shardContainer == null) {
                ActionListener.completeWith(l, () -> ObjectStoreService.IndexingShardState.EMPTY);
                return;
            }
            ObjectStoreService.readIndexingShardState(
                IndexBlobStoreCacheDirectory.unwrapDirectory(indexShard.store().directory()),
                IOContext.DEFAULT,
                shardContainer,
                indexShard.getOperationPrimaryTerm(),
                threadPool,
                statelessCommitService.useReplicatedRanges(),
                bccHeaderReadExecutor,
                true,
                statelessCommitService.getSourceBlobsEntry(indexShard.shardId()),
                l
            );
        }).<Void>andThen((l, state) -> recoverBatchedCompoundCommitOnIndexShard(indexShard, state, l)).addListener(listener);
    }

    private void recoverBatchedCompoundCommitOnIndexShard(
        IndexShard indexShard,
        ObjectStoreService.IndexingShardState indexingShardState,
        ActionListener<Void> listener
    ) {
        ActionListener.completeWith(listener, () -> {
            assert ThreadPool.assertCurrentThreadPool(ThreadPool.Names.GENERIC);

            var store = indexShard.store();
            var indexDirectory = IndexDirectory.unwrapDirectory(store.directory());
            var batchedCompoundCommit = indexingShardState.latestCommit();
            logBootstrapping(indexShard, batchedCompoundCommit);

            if (batchedCompoundCommit != null) {
                var recoveryCommit = batchedCompoundCommit.lastCompoundCommit();
                var blobFileRanges = indexingShardState.blobFileRanges();
                assert blobFileRanges.keySet().containsAll(recoveryCommit.commitFiles().keySet())
                    || statelessCommitService.useReplicatedRanges() == false;

                indexDirectory.updateRecoveryCommit(
                    recoveryCommit.generation(),
                    recoveryCommit.nodeEphemeralId(),
                    recoveryCommit.translogRecoveryStartFile(),
                    recoveryCommit.getAllFilesSizeInBytes(),
                    blobFileRanges
                );
                if (recoveryCommit.hollow() == false) {
                    // We must use a copied instance for warming as the index directory will move forward with new commits
                    var warmingDirectory = indexDirectory.getBlobStoreCacheDirectory().createNewBlobStoreCacheDirectoryForWarming();
                    warmingDirectory.updateMetadata(blobFileRanges, recoveryCommit.getAllFilesSizeInBytes());
                    warmingService.warmCacheForShardRecovery(INDEXING, indexShard, recoveryCommit, warmingDirectory, null);
                }
            }
            final var segmentInfos = SegmentInfos.readLatestCommit(indexDirectory);
            final var translogUUID = segmentInfos.userData.get(Translog.TRANSLOG_UUID_KEY);
            final var checkPoint = segmentInfos.userData.get(SequenceNumbers.LOCAL_CHECKPOINT_KEY);
            if (translogUUID != null) {
                Translog.createEmptyTranslog(
                    indexShard.shardPath().resolveTranslog(),
                    indexShard.shardId(),
                    checkPoint == null ? SequenceNumbers.UNASSIGNED_SEQ_NO : Long.parseLong(checkPoint),
                    indexShard.getPendingPrimaryTerm(),
                    translogUUID,
                    null
                );
            } else {
                bootstrap(indexShard, store);
            }

            if (batchedCompoundCommit != null) {
                assert batchedCompoundCommit.shardId().equals(indexShard.shardId())
                    || indexShard.routingEntry()
                        .recoverySource() instanceof RecoverySource.ReshardSplitRecoverySource reshardSplitRecoverySource
                        && reshardSplitRecoverySource.getSourceShardId().equals(batchedCompoundCommit.shardId())
                    : batchedCompoundCommit.shardId() + " vs " + indexShard.shardId();

                statelessCommitService.markRecoveredBcc(indexShard.shardId(), batchedCompoundCommit, indexingShardState.otherBlobs());
            }
            statelessCommitService.addConsumerForNewUploadedBcc(indexShard.shardId(), info -> {
                Set<String> uploadedFiles = info.uploadedBcc()
                    .compoundCommits()
                    .stream()
                    .flatMap(f -> f.commitFiles().keySet().stream())
                    .collect(Collectors.toSet());
                indexDirectory.updateCommit(
                    info.uploadedBcc().lastCompoundCommit().generation(),
                    info.uploadedBcc().lastCompoundCommit().getAllFilesSizeInBytes(),
                    uploadedFiles,
                    info.blobFileRanges()
                );
            });

            statelessCommitService.addConsumerForNewUploadedBcc(
                indexShard.shardId(),
                info -> translogReplicator.markShardCommitUploaded(indexShard.shardId(), info.translogReleaseEndFile())
            );
            return null;
        });
    }

    private void beforeRecoveryOnSearchShard(IndexShard indexShard, BlobContainer blobContainer, ActionListener<Void> listener)
        throws IOException {
        assert indexShard.store().refCount() > 0 : indexShard.shardId();
        assert blobContainer != null : indexShard.routingEntry();

        final var searchDirectory = SearchDirectory.unwrapDirectory(indexShard.store().directory());
        final var batchedCompoundCommit = objectStoreService.readSearchShardState(
            blobContainer,
            searchDirectory,
            indexShard.getOperationPrimaryTerm()
        );
        assert batchedCompoundCommit == null || batchedCompoundCommit.shardId().equals(indexShard.shardId())
            : batchedCompoundCommit.shardId() + " != " + indexShard.shardId();

        recoveryCommitRegistrationHandler.register(
            batchedCompoundCommit != null ? batchedCompoundCommit.primaryTermAndGeneration() : PrimaryTermAndGeneration.ZERO,
            batchedCompoundCommit != null
                ? batchedCompoundCommit.lastCompoundCommit().primaryTermAndGeneration()
                : PrimaryTermAndGeneration.ZERO,
            indexShard.shardId(),
            listener.<RegisterCommitResponse>delegateFailure((l, response) -> {
                var lastUploaded = response.getLatestUploadedBatchedCompoundCommitTermAndGen();
                var nodeId = response.getNodeId();
                assert nodeId != null : response;

                final StatelessCompoundCommit compoundCommit;
                if (response.getCompoundCommit() == null) {
                    // If the indexing shard provided no compound commit to recover from, then the last uploaded BCC term/gen returned
                    // should be equal to zero indicated the indexing shard's engine is null or is a NoOpEngine
                    assert PrimaryTermAndGeneration.ZERO.equals(lastUploaded) : lastUploaded;

                    logBootstrapping(indexShard, batchedCompoundCommit);
                    // If there is no batched compound commit found in the object store, then recover from an empty commit
                    if (batchedCompoundCommit == null) {
                        l.onResponse(null);
                        return;
                    }

                    // Otherwise recover from the compound commit found in the object store
                    // TODO Should we revisit this? the indexing shard does not know about the commits used by this search shard
                    // until the next new commit notification.
                    compoundCommit = batchedCompoundCommit.lastCompoundCommit();
                    lastUploaded = batchedCompoundCommit.primaryTermAndGeneration();
                } else {
                    compoundCommit = response.getCompoundCommit();
                    logBootstrapping(indexShard, compoundCommit, lastUploaded);
                }

                assert batchedCompoundCommit == null
                    || batchedCompoundCommit.lastCompoundCommit()
                        .primaryTermAndGeneration()
                        .onOrBefore(compoundCommit.primaryTermAndGeneration());

                searchDirectory.updateLatestUploadedBcc(lastUploaded);
                searchDirectory.updateLatestCommitInfo(compoundCommit.primaryTermAndGeneration(), nodeId);

                SubscribableListener.<Tuple<Map<String, BlobFileRanges>, Map<BlobFile, Integer>>>newForked(l2 -> {
                    if (useInternalFilesReplicatedContentForSearchShards) {
                        Map<String, BlobFileRanges> blobFileRanges = ConcurrentCollections.newConcurrentMap();
                        Map<BlobFile, Integer> regionsToWarm = ConcurrentCollections.newConcurrentMap();
                        ObjectStoreService.readReferencedCompoundCommitsUsingCache(
                            compoundCommit,
                            batchedCompoundCommit,
                            searchDirectory,
                            IOContext.DEFAULT,
                            bccHeaderReadExecutor,
                            referencedCompoundCommit -> {
                                blobFileRanges.putAll(
                                    computeBlobFileRanges(
                                        true,
                                        referencedCompoundCommit.statelessCompoundCommitReference().compoundCommit(),
                                        referencedCompoundCommit.statelessCompoundCommitReference().headerOffsetInTheBccBlobFile(),
                                        referencedCompoundCommit.referencedInternalFiles()
                                    )
                                );
                                regionsToWarm.compute(
                                    referencedCompoundCommit.statelessCompoundCommitReference().bccBlobFile(),
                                    (blobFile, maxRegionToWarm) -> {
                                        var regionIdx = warmingService.regionsToWarmForCC(referencedCompoundCommit);
                                        return maxRegionToWarm == null ? regionIdx : Math.max(maxRegionToWarm, regionIdx);
                                    }
                                );
                            },
                            l2.map(aVoid -> new Tuple<>(blobFileRanges, regionsToWarm))
                        );
                    } else {
                        l2.onResponse(null);
                    }
                }).addListener(l.delegateFailureAndWrap((l3, blobFileRangesAndRegionsToWarm) -> {
                    if (blobFileRangesAndRegionsToWarm != null) {
                        searchDirectory.updateCommit(compoundCommit, blobFileRangesAndRegionsToWarm.v1());
                        warmingService.warmCacheForShardRecovery(
                            SEARCH,
                            indexShard,
                            compoundCommit,
                            searchDirectory,
                            blobFileRangesAndRegionsToWarm.v2()
                        );
                    } else {
                        searchDirectory.updateCommit(compoundCommit);
                        warmingService.warmCacheForShardRecovery(SEARCH, indexShard, compoundCommit, searchDirectory, null);
                    }
                    // this goes async (aka "offline warming")
                    assert indexShard.store().refCount() > 0 : indexShard.shardId();
                    l3.onResponse(null);
                }));
            })
        );
    }

    @Override
    public void afterIndexShardRecovery(IndexShard indexShard, ActionListener<Void> listener) {
        ActionListener.run(listener, l -> {
            if (indexShard.routingEntry().isPromotableToPrimary()) {
                Engine engineOrNull = indexShard.getEngineOrNull();

                IndexSettings indexSettings = indexShard.indexSettings();
                IndexReshardingMetadata reshardingMetadata = indexSettings.getIndexMetadata().getReshardingMetadata();
                // TODO with this implementation we will sometimes run below calls on stateless_upload_prewarm thread pool
                // due to how statelessCommitService.addListenerForUploadedGeneration works.
                if (IndexReshardingMetadata.isSplitSource(indexShard.shardId(), reshardingMetadata)) {
                    l = l.delegateFailure(
                        (toWrap, unused) -> splitSourceService.afterSplitSourceIndexShardRecovery(indexShard, reshardingMetadata, toWrap)
                    );
                }
                if (engineOrNull instanceof IndexEngine engine) {
                    long currentGeneration = engine.getCurrentGeneration();
                    if (currentGeneration > statelessCommitService.getRecoveredGeneration(indexShard.shardId())) {
                        ShardId shardId = indexShard.shardId();
                        statelessCommitService.addListenerForUploadedGeneration(shardId, engine.getCurrentGeneration(), l);
                    } else {
                        engine.flush(true, true, l.map(f -> null));
                    }
                } else if (engineOrNull instanceof HollowIndexEngine) {
                    hollowShardsService.addHollowShard(indexShard, "recovery");
                    // Evict the recovery BCC blob, since it won't be read again, in order to create space for new cache entries.
                    // Note: we run prewarming asynchronously. It's unlikely but possible to have it race with eviction, and that may mean
                    // the cache entry ultimately stays in the cache. But because it should be rare, we do not optimize further for this.
                    cacheService.forceEvict(indexShard.shardId(), k -> true);
                    l.onResponse(null);
                } else if (engineOrNull == null) {
                    throw new AlreadyClosedException("engine is closed");
                } else {
                    assert engineOrNull instanceof NoOpEngine;
                    l.onResponse(null);
                }
            } else {
                final Engine engineOrNull = indexShard.getEngineOrNull();
                if (engineOrNull instanceof SearchEngine searchEngine) {
                    assert indexShard.state() == IndexShardState.RECOVERING
                        : "expected index in recovering shard state but is: " + indexShard.state();
                    assert indexShard.routingEntry().state() == ShardRoutingState.INITIALIZING
                        : "expected initializing shard routing state but is: " + indexShard.state();
                    indexShard.updateGlobalCheckpointOnReplica(searchEngine.getLastSyncedGlobalCheckpoint(), "search shard recovery");
                    searchEngine.afterRecovery();
                    l.onResponse(null);
                } else if (engineOrNull == null) {
                    throw new AlreadyClosedException("engine is closed");
                } else {
                    assert engineOrNull instanceof NoOpEngine;
                    l.onResponse(null);
                }
            }
        });
    }

    @Override
    public void beforeIndexShardMutableOperation(IndexShard indexShard, boolean permitAcquired, ActionListener<Void> listener) {
        hollowShardsService.onMutableOperation(indexShard, permitAcquired, listener);
    }
}
